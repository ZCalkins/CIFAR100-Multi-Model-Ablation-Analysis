general:
  device: "cuda" if torch.cuda.is_available() else "cpu"
  seed: 42
  num_workers: 4

experiment:
  name: "CNN_Hyperparameter_Tuning_CIFAR100"
  description: "CNN hyperparameter tuning on the CIFAR100 dataset"
  save_dir: "results/hyperparameter_tuning/cnn"
  log_dir: "logs/tensorboard/hyperparameter_tuning"
  checkpoints_dir: "checkpoints/hyperparameter_tuning/cnn"
  resume_checkpoint: null
  # Change this to the path of the checkpoint file to resume

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_to_file: False
  log_to_console: False

monitoring:
  tensorboard: True

checkpointing:
  save_best_only: True
  monitor_metric: "val_loss"
  save_freq: "epoch"
  max_checkpoints: 5

early_stopping:
  enabled: True
  monitor: "val_loss"
  patience: 5
  min_delta: 0.001

hyperparameter_optimization:
  enabled: True
  optimizer: "Optuna"
  n_trials: 100
  pruner: "SuccessiveHalvingPruner"
  direction: "minimize"

evaluation:
  metrics: ["accuracy", "precision", "recall", "f1"]
  batch_size: 32
  num_workers: 4
  verbose: True

misc:
  debug: False
  use_mixed_precision: True
  deterministic: False
  additional_config: "path_to_additional_config.yaml"
  smaller_dataset: True
  num_epochs_debug: 5
  profiler_enabled: True
